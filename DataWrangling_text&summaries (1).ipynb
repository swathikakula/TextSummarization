{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Aug 01 01:11:02 2016\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^.a-zA-z0-9\\s]' if remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern,'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, contraction_expansion = True, text_lower_case = True, special_char_removal = True,remove_digits=True):\n",
    "\n",
    "    normalized_corpus = []\n",
    "    for doc in corpus:\n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        if special_char_removal:\n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits = remove_digits)\n",
    "        doc = re.sub(' +',' ', doc)\n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on problem proposal associated to Text summarization, various dataset was analyzed. Finally given wide range of information within new articles, CNN/Dalymail dataset was chosen. It contains various news articles ranging from political, sports, fashion, science areas. Every article contains detailed text along with few line summaries. This will be used for supervised model where algorithm will be trained and accuracies are determined. Once the model behavior is satisfactory, it will generate summary on unseen text.\n",
    "The data was extracted in form of each story file through website.Once the zipped files are manually downloaded, each story file was read one after another.\n",
    "Text pre processing techniques such as Replacing new line characters, and segregating text and summary was performed. As per the dataset summary was prefixed with @highlight. So regex was used to split summary and detailed text separately for every article. These are then loaded into Dataframe for further cleaning.\n",
    "Pre processing steps such as lower case conversion, removal of special characters and expansion of contractions was performed. These steps are applied on entire dataframe containing text and summary segregate as 2 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicholas Levene must pay the nominal sum becau...</td>\n",
       "      <td>By Alex Ward  A city trader who conned million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bella Rodriguez-Torres was diagnosed with stag...</td>\n",
       "      <td>By  Helen Pow  PUBLISHED:  09:16 EST, 29 May 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jessica Weller set off on an epic adventure of...</td>\n",
       "      <td>She posed in front of the Eiffel Tower in Pari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The British No 1 defeated the Canadian youngst...</td>\n",
       "      <td>Andy Murray marched into the third round of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photographer Amos Chapple took this stunning c...</td>\n",
       "      <td>It is perceived as one of the most introverted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sir Irvine Patnick accused Liverpool fans of c...</td>\n",
       "      <td>By  Lucy Osborne and Rosie Taylor  PUBLISHED: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dubbed 'Scape', the technology can help unlock...</td>\n",
       "      <td>Scientists have developed a new microscope tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ryan Aprea, two, was born at just 25 weeks and...</td>\n",
       "      <td>The moment when a young boy with severe hearin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Actor Stephen Collins has hit rock bottom sinc...</td>\n",
       "      <td>The family of Stephen Collins fear the 7th Hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pochettino has left Southampton to join Totten...</td>\n",
       "      <td>Southampton midfielder Steven Davis was sad to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Summary  \\\n",
       "0  Nicholas Levene must pay the nominal sum becau...   \n",
       "1  Bella Rodriguez-Torres was diagnosed with stag...   \n",
       "2  Jessica Weller set off on an epic adventure of...   \n",
       "3  The British No 1 defeated the Canadian youngst...   \n",
       "4  Photographer Amos Chapple took this stunning c...   \n",
       "5  Sir Irvine Patnick accused Liverpool fans of c...   \n",
       "6  Dubbed 'Scape', the technology can help unlock...   \n",
       "7  Ryan Aprea, two, was born at just 25 weeks and...   \n",
       "8  Actor Stephen Collins has hit rock bottom sinc...   \n",
       "9  Pochettino has left Southampton to join Totten...   \n",
       "\n",
       "                                                Text  \n",
       "0  By Alex Ward  A city trader who conned million...  \n",
       "1  By  Helen Pow  PUBLISHED:  09:16 EST, 29 May 2...  \n",
       "2  She posed in front of the Eiffel Tower in Pari...  \n",
       "3  Andy Murray marched into the third round of th...  \n",
       "4  It is perceived as one of the most introverted...  \n",
       "5  By  Lucy Osborne and Rosie Taylor  PUBLISHED: ...  \n",
       "6  Scientists have developed a new microscope tha...  \n",
       "7  The moment when a young boy with severe hearin...  \n",
       "8  The family of Stephen Collins fear the 7th Hea...  \n",
       "9  Southampton midfielder Steven Davis was sad to...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = r'C:\\Users\\sa182c\\Desktop\\personal\\python\\projects\\dailymail\\dailymail\\articles'\n",
    "t_lst = []\n",
    "s_lst = []\n",
    "for file in os.listdir(directory): #t_lst = []# s_lst = []\n",
    "    data=open(os.path.join(directory,file), encoding = 'utf-8', errors='ignore')\n",
    "    \n",
    "    x = [line.strip('\\n').replace(\"\\n\",\" \") for line in re.split(r'@highlight', data.read() ,re.M)]\n",
    "    \n",
    "    summary = x[1: ]\n",
    "    text = x[0]\n",
    "   \n",
    "    summary_str = '.'\n",
    "    t_lst.append(text)\n",
    "    s_lst.append(summary_str.join(summary))\n",
    "    \n",
    "    \n",
    "\n",
    "news_df = pd.DataFrame(list(zip(s_lst, t_lst)), columns = ['Summary', 'Text'])\n",
    "\n",
    "\n",
    "news_df2 = pd.DataFrame(list(zip(normalize_corpus(news_df['Summary']),normalize_corpus(news_df['Text']))),columns = ['clean_sum','clean_text'])\n",
    "news_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
